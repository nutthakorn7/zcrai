package scheduler

import (
	"context"
	"crypto/md5"
	"encoding/hex"
	"fmt"
	"sync"
	"time"

	"github.com/robfig/cron/v3"
	"github.com/zrd4y/zcrAI/collector/internal/clickhouse"
	"github.com/zrd4y/zcrAI/collector/internal/config"
	"github.com/zrd4y/zcrAI/collector/internal/providers/crowdstrike"
	"github.com/zrd4y/zcrAI/collector/internal/providers/sentinelone"
	"github.com/zrd4y/zcrAI/collector/internal/publisher"
	"github.com/zrd4y/zcrAI/collector/internal/state"
	"github.com/zrd4y/zcrAI/collector/pkg/models"
	"go.uber.org/zap"
)

// createURLHash à¸ªà¸£à¹‰à¸²à¸‡ MD5 hash à¸‚à¸­à¸‡ URL à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ URL à¹€à¸”à¸´à¸¡à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
func createURLHash(url string) string {
	hash := md5.Sum([]byte(url))
	return hex.EncodeToString(hash[:])
}

// Scheduler à¸ˆà¸±à¸”à¸à¸²à¸£ scheduled jobs
type Scheduler struct {
	cron       *cron.Cron
	config     *config.Config
	publisher  *publisher.Publisher
	state      *state.State
	logger     *zap.Logger
	clickhouse *clickhouse.Client // ClickHouse client à¸ªà¸³à¸«à¸£à¸±à¸š query existing data
	mu         sync.Mutex
	running    bool
	// â­ Context management à¸ªà¸³à¸«à¸£à¸±à¸š cancel sync à¹€à¸¡à¸·à¹ˆà¸­ Integration à¸–à¸¹à¸à¸¥à¸š
	syncContexts map[string]context.CancelFunc // key = integrationID
	syncMu       sync.RWMutex
}

// NewScheduler à¸ªà¸£à¹‰à¸²à¸‡ Scheduler à¹ƒà¸«à¸¡à¹ˆ
func NewScheduler(cfg *config.Config, pub *publisher.Publisher) *Scheduler {
	// à¸ªà¸£à¹‰à¸²à¸‡ State à¸”à¹‰à¸§à¸¢ API URL à¹à¸¥à¸° Collector Key
	st := state.NewState(cfg.ElysiaURL, cfg.CollectorAPIKey)

	// à¸ªà¸£à¹‰à¸²à¸‡ ClickHouse client à¸ªà¸³à¸«à¸£à¸±à¸š query existing data (Best Practice: à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ duplicate)
	chClient, err := clickhouse.NewClient(clickhouse.Config{
		Host:     cfg.ClickHouseHost,
		Port:     cfg.ClickHousePort,
		Database: cfg.ClickHouseDB,
		Username: cfg.ClickHouseUser,
		Password: cfg.ClickHousePassword,
	}, cfg.Logger)
	if err != nil {
		cfg.Logger.Warn("Failed to connect to ClickHouse for existing data check", zap.Error(err))
		// à¹„à¸¡à¹ˆ fatal - à¸¢à¸±à¸‡à¸—à¸³à¸‡à¸²à¸™à¹„à¸”à¹‰à¹à¸•à¹ˆà¸­à¸²à¸ˆà¸¡à¸µ duplicate à¸–à¹‰à¸² re-add integration
	}

	return &Scheduler{
		cron:         cron.New(cron.WithSeconds()),
		config:       cfg,
		publisher:    pub,
		state:        st,
		logger:       cfg.Logger,
		clickhouse:   chClient,
		syncContexts: make(map[string]context.CancelFunc),
	}
}

// Start à¹€à¸£à¸´à¹ˆà¸¡ scheduler
func (s *Scheduler) Start() {
	s.mu.Lock()
	defer s.mu.Unlock()

	if s.running {
		return
	}

	// Run initial check immediately (in background)
	go s.checkAndRunFullSync()

	// Schedule SentinelOne collection à¸—à¸¸à¸ 5 à¸™à¸²à¸—à¸µ
	s.cron.AddFunc("0 */5 * * * *", func() {
		s.collectSentinelOne(false) // Regular collection (7 days)
	})

	// Schedule CrowdStrike collection à¸—à¸¸à¸ 5 à¸™à¸²à¸—à¸µ
	s.cron.AddFunc("0 */5 * * * *", func() {
		s.collectCrowdStrike(false)
	})

	s.cron.Start()
	s.running = true
	s.logger.Info("Scheduler started")
}

// Stop à¸«à¸¢à¸¸à¸” scheduler
func (s *Scheduler) Stop() {
	s.mu.Lock()
	defer s.mu.Unlock()

	if !s.running {
		return
	}

	// Cancel all running syncs
	s.syncMu.Lock()
	for id, cancel := range s.syncContexts {
		s.logger.Info("Cancelling sync on shutdown", zap.String("integrationId", id))
		cancel()
	}
	s.syncContexts = make(map[string]context.CancelFunc)
	s.syncMu.Unlock()

	s.cron.Stop()
	s.running = false
	s.logger.Info("Scheduler stopped")
}

// CancelSync à¸«à¸¢à¸¸à¸” sync à¸‚à¸­à¸‡ Integration à¸—à¸µà¹ˆà¸£à¸°à¸šà¸¸ (à¹€à¸£à¸µà¸¢à¸à¹€à¸¡à¸·à¹ˆà¸­ Integration à¸–à¸¹à¸à¸¥à¸š)
func (s *Scheduler) CancelSync(integrationID string) {
	s.syncMu.Lock()
	defer s.syncMu.Unlock()

	if cancel, ok := s.syncContexts[integrationID]; ok {
		s.logger.Info("Cancelling sync for deleted integration",
			zap.String("integrationId", integrationID))
		cancel()
		delete(s.syncContexts, integrationID)
	}
}

// TriggerReload à¸šà¸­à¸ scheduler à¹ƒà¸«à¹‰ reload config à¹à¸¥à¸° sync à¹ƒà¸«à¸¡à¹ˆà¸«à¸¥à¸±à¸‡à¸£à¸­ 30 à¸§à¸´à¸™à¸²à¸—à¸µ
func (s *Scheduler) TriggerReload(integrationID string) {
	s.logger.Info("ðŸ”„ [TriggerReload] Integration updated - will resync with new config in 30 seconds",
		zap.String("integrationId", integrationID))

	// â­ Cancel current sync à¸–à¹‰à¸²à¸à¸³à¸¥à¸±à¸‡à¸—à¸³à¸‡à¸²à¸™à¸­à¸¢à¸¹à¹ˆ à¹€à¸žà¸·à¹ˆà¸­à¸«à¸¢à¸¸à¸”à¸à¸²à¸£à¹ƒà¸Šà¹‰ config à¹€à¸à¹ˆà¸²
	s.syncMu.Lock()
	if cancel, ok := s.syncContexts[integrationID]; ok {
		s.logger.Info("ðŸ›‘ [TriggerReload] Cancelling current sync (old config)",
			zap.String("integrationId", integrationID))
		cancel()
		delete(s.syncContexts, integrationID)
	}
	s.syncMu.Unlock()

	// â­ à¸£à¸­ 30 à¸§à¸´à¸™à¸²à¸—à¸µà¸žà¸£à¹‰à¸­à¸¡ progress countdown
	go func() {
		totalWait := 30
		for i := totalWait; i > 0; i-- {
			s.logger.Info("â³ [TriggerReload] Countdown to resync",
				zap.String("integrationId", integrationID),
				zap.Int("secondsRemaining", i))
			time.Sleep(1 * time.Second)
		}

		s.logger.Info("ðŸš€ [TriggerReload] Starting collection with new config",
			zap.String("integrationId", integrationID))

		// à¸£à¸±à¸™ collection à¹ƒà¸«à¸¡à¹ˆ (à¸ˆà¸°à¸”à¸¶à¸‡ config à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸ API)
		if err := s.collectSentinelOne(false); err != nil {
			s.logger.Error("âŒ [TriggerReload] S1 collection failed", zap.Error(err))
		} else {
			s.logger.Info("âœ… [TriggerReload] S1 collection completed with new config")
		}

		if err := s.collectCrowdStrike(false); err != nil {
			s.logger.Error("âŒ [TriggerReload] CrowdStrike collection failed", zap.Error(err))
		} else {
			s.logger.Info("âœ… [TriggerReload] CrowdStrike collection completed with new config")
		}
	}()
}

// RunNow à¸£à¸±à¸™ collection à¸—à¸±à¸™à¸—à¸µ
func (s *Scheduler) RunNow(source string) error {
	switch source {
	case "sentinelone":
		return s.collectSentinelOne(false)
	case "crowdstrike":
		return s.collectCrowdStrike(false)
	case "all":
		if err := s.collectSentinelOne(false); err != nil {
			s.logger.Error("S1 collection failed", zap.Error(err))
		}
		if err := s.collectCrowdStrike(false); err != nil {
			s.logger.Error("CrowdStrike collection failed", zap.Error(err))
		}
		return nil
	default:
		s.logger.Warn("Unknown source", zap.String("source", source))
		return nil
	}
}

// checkAndRunFullSync à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š state à¹à¸¥à¸°à¸£à¸±à¸™ Full Sync à¸–à¹‰à¸²à¸ˆà¸³à¹€à¸›à¹‡à¸™
func (s *Scheduler) checkAndRunFullSync() {
	s.logger.Info("Checking full sync status...")

	// â­ à¸›à¸£à¸±à¸šà¹ƒà¸Šà¹‰ URL hash-based state - logic à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ collectSentinelOne à¹à¸¥à¸° collectCrowdStrike à¹à¸¥à¹‰à¸§
	// à¹€à¸žà¸µà¸¢à¸‡à¹à¸„à¹ˆ trigger collection à¹à¸¥à¸°à¹ƒà¸«à¹‰ logic à¸‚à¹‰à¸²à¸‡à¹ƒà¸™à¸ˆà¸±à¸”à¸à¸²à¸£à¹€à¸­à¸‡

	// Trigger collection with forceFullSync=true (logic inside will check state again or force it)
	// Actually, to be precise, we should pass a map of tenants to full sync.
	// But let's simplify: collectSentinelOne(true) will use 365 days for ALL tenants.
	// Better: collectSentinelOne checks state for each tenant.

	s.collectSentinelOne(false) // Just run normal collection, but inside we check state
	s.collectCrowdStrike(false) // Same for CrowdStrike
}

// collectSentinelOne à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ SentinelOne à¸—à¸¸à¸ tenant
func (s *Scheduler) collectSentinelOne(forceFullSync bool) error {
	s.logger.Info("Starting SentinelOne collection", zap.Bool("forceFullSync", forceFullSync))

	// à¸”à¸¶à¸‡ integrations à¸ˆà¸²à¸ Elysia
	integrations, err := s.config.FetchIntegrations("sentinelone")
	if err != nil {
		s.logger.Error("Failed to fetch S1 integrations", zap.Error(err))
		return err
	}

	for _, integration := range integrations {
		// Determine time range based on state
		endTime := time.Now().UTC()
		var startTime time.Time
		isFullSync := false

		// Parse config à¹€à¸žà¸·à¹ˆà¸­à¸”à¸¶à¸‡ URL à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸£à¹‰à¸²à¸‡ hash
		cfg, err := config.ParseS1Config(integration.Config)
		if err != nil {
			s.logger.Error("Failed to parse S1 config",
				zap.String("integrationId", integration.ID),
				zap.Error(err))
			continue
		}

		// à¸ªà¸£à¹‰à¸²à¸‡ URL hash à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸Šà¹‡à¸„ data
		urlHash := createURLHash(cfg.BaseURL)
		provider := "sentinelone"

		// Check state à¸ˆà¸²à¸ API (PostgreSQL)
		checkpoint := s.state.GetCheckpoint(integration.TenantID, provider, urlHash)

		// à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ integration à¹ƒà¸«à¸¡à¹ˆ (pending) à¸«à¸£à¸·à¸­ forceFullSync à¸«à¸£à¸·à¸­à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¹€à¸„à¸¢ sync
		needsFullSync := integration.LastSyncStatus == "pending" ||
			!s.state.HasFullSync(integration.TenantID, provider, urlHash) ||
			forceFullSync

		if needsFullSync {
			// â­ Option C (Best Practice): à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸² URL à¹€à¸”à¸´à¸¡ + à¸¡à¸µ data à¸„à¸£à¸šà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
			foundExisting := false
			dataComplete := false

			if s.clickhouse != nil {
				// à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¸¡à¸µ data à¸ˆà¸²à¸ URL à¸™à¸µà¹‰à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
				existingTimestamp, count, err := s.clickhouse.GetLatestTimestampByURL(integration.TenantID, provider, urlHash)
				if err != nil {
					s.logger.Warn("Failed to check existing data by URL in ClickHouse", zap.Error(err))
				} else if !existingTimestamp.IsZero() && count > 0 {
					// à¸¡à¸µ data à¸ˆà¸²à¸ URL à¹€à¸”à¸´à¸¡ â†’ à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¸„à¸£à¸šà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
					foundExisting = true

					// à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸² data à¸„à¸£à¸šà¸–à¹‰à¸§à¸™ (oldest event à¸„à¸£à¸­à¸šà¸„à¸¥à¸¸à¸¡ 365 à¸§à¸±à¸™)
					dataComplete, _, err = s.clickhouse.CheckDataCompleteness(integration.TenantID, provider, 365)
					if err != nil {
						s.logger.Warn("Failed to check data completeness", zap.Error(err))
					}

					if dataComplete {
						// URL à¹€à¸”à¸´à¸¡ + data à¸„à¸£à¸š â†’ Incremental sync à¸ˆà¸²à¸ timestamp à¸¥à¹ˆà¸²à¸ªà¸¸à¸”
						startTime = existingTimestamp.Add(1 * time.Second)
						s.logger.Info("URL match + data complete â†’ Incremental sync",
							zap.String("tenantId", integration.TenantID),
							zap.String("urlHash", urlHash),
							zap.Uint64("existingCount", count),
							zap.Time("startFrom", startTime))
					} else {
						// URL à¹€à¸”à¸´à¸¡ à¹à¸•à¹ˆ data à¹„à¸¡à¹ˆà¸„à¸£à¸š â†’ Full sync à¹ƒà¸«à¸¡à¹ˆ (ReplacingMergeTree à¸ˆà¸° dedupe)
						foundExisting = false
						s.logger.Info("URL match but data incomplete â†’ Full sync",
							zap.String("tenantId", integration.TenantID),
							zap.String("urlHash", urlHash),
							zap.Uint64("existingCount", count))
					}
				}
			}

			if !foundExisting {
				// à¹„à¸¡à¹ˆà¸¡à¸µ data à¸ˆà¸²à¸ URL à¸™à¸µà¹‰ à¸«à¸£à¸·à¸­ data à¹„à¸¡à¹ˆà¸„à¸£à¸š â†’ Full sync 30 à¸§à¸±à¸™ (à¸¥à¸”à¸ˆà¸²à¸ 365 à¹€à¸žà¸·à¹ˆà¸­à¸¥à¸” CPU)
				startTime = endTime.AddDate(0, 0, -30)
				isFullSync = true
				s.logger.Info("Performing Full Sync (30 days)",
					zap.String("tenantId", integration.TenantID),
					zap.String("urlHash", urlHash),
					zap.String("reason", integration.LastSyncStatus))
			}
		} else {
			// Incremental sync à¸ˆà¸²à¸ checkpoint (à¸ˆà¸²à¸ API/PostgreSQL)
			if checkpoint != nil && !checkpoint.IsZero() {
				startTime = checkpoint.Add(1 * time.Second) // +1 sec à¹€à¸žà¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸”à¸¶à¸‡ event à¹€à¸”à¸´à¸¡à¸‹à¹‰à¸³
				s.logger.Info("Resuming from checkpoint",
					zap.String("tenantId", integration.TenantID),
					zap.Time("checkpoint", startTime))
			} else {
				// Fallback to config lookback (7 days)
				startTime = endTime.AddDate(0, 0, -s.config.LookbackDays)
				s.logger.Info("No checkpoint found, using default lookback",
					zap.String("tenantId", integration.TenantID),
					zap.Int("days", s.config.LookbackDays))
			}
		}

		// à¸ªà¸£à¹‰à¸²à¸‡ client à¸žà¸£à¹‰à¸­à¸¡ Integration info
		integrationName := integration.Name
		if integrationName == "" {
			integrationName = fmt.Sprintf("%s-%s", integration.Provider, integration.ID[:8])
		}
		s1Client := sentinelone.NewS1Client(integration.TenantID, integration.ID, integrationName, cfg, s.logger)

		// â­ à¸ªà¸£à¹‰à¸²à¸‡ context à¸ªà¸³à¸«à¸£à¸±à¸š cancel sync (à¹€à¸¡à¸·à¹ˆà¸­ Integration à¸–à¸¹à¸à¸¥à¸š)
		ctx, cancel := context.WithCancel(context.Background())
		s.syncMu.Lock()
		s.syncContexts[integration.ID] = cancel
		s.syncMu.Unlock()

		// Cleanup context à¹€à¸¡à¸·à¹ˆà¸­à¸ˆà¸š sync
		defer func(id string) {
			s.syncMu.Lock()
			delete(s.syncContexts, id)
			s.syncMu.Unlock()
			cancel()
		}(integration.ID)

		// Capture urlHash and provider for callbacks
		currentURLHash := urlHash
		currentProvider := provider

		// Callback à¸ªà¸³à¸«à¸£à¸±à¸š save checkpoint à¸«à¸¥à¸±à¸‡à¸ˆà¸š sync (à¸œà¹ˆà¸²à¸™ API)
		onChunkComplete := func(chunkEndTime time.Time) {
			if err := s.state.UpdateCheckpoint(integration.TenantID, currentProvider, currentURLHash, chunkEndTime); err != nil {
				s.logger.Error("Failed to save checkpoint", zap.Error(err))
			}
		}

		// Callback à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¹ˆà¸‡ events à¹„à¸› Vector à¸—à¸±à¸™à¸—à¸µà¹à¸•à¹ˆà¸¥à¸° page (Streaming)
		onPageEvents := func(events []models.UnifiedEvent) error {
			return s.publisher.PublishBatch(events, 5000) // â­ Increased from 500 to reduce ClickHouse CPU
		}

		// â­ à¸”à¸¶à¸‡ FetchSettings à¸ˆà¸²à¸ config (User à¸ªà¸²à¸¡à¸²à¸£à¸– custom à¹„à¸”à¹‰)
		fetchSettings := cfg.FetchSettings
		if fetchSettings == nil {
			s.logger.Info("FetchSettings is nil, using defaults")
			// Default values à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µ fetchSettings
			fetchSettings = &config.S1FetchSettings{
				Threats:    &config.FetchSettingItem{Enabled: true, Days: 365},
				Activities: &config.FetchSettingItem{Enabled: true, Days: 120},
				Alerts:     &config.FetchSettingItem{Enabled: true, Days: 365},
			}
		} else {
			// â­ Log fetchSettings à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸£à¸±à¸š
			s.logger.Info("FetchSettings from config",
				zap.Bool("threatsEnabled", fetchSettings.Threats != nil && fetchSettings.Threats.Enabled),
				zap.Int("threatsDays", func() int {
					if fetchSettings.Threats != nil {
						return fetchSettings.Threats.Days
					}
					return 0
				}()),
				zap.Bool("activitiesEnabled", fetchSettings.Activities != nil && fetchSettings.Activities.Enabled),
				zap.Int("activitiesDays", func() int {
					if fetchSettings.Activities != nil {
						return fetchSettings.Activities.Days
					}
					return 0
				}()),
			)
		}

		// à¸”à¸¶à¸‡ Threats (à¸–à¹‰à¸² enabled)
		threatCount := 0
		if fetchSettings.Threats == nil || fetchSettings.Threats.Enabled {
			threatDays := 365
			if fetchSettings.Threats != nil {
				threatDays = fetchSettings.Threats.Days
			}

			threatStartTime := startTime
			if isFullSync {
				threatStartTime = endTime.AddDate(0, 0, -threatDays)
			}

			s.logger.Info("Fetching S1 threats",
				zap.Int("days", threatDays),
				zap.Time("from", threatStartTime))

			count, err := s1Client.FetchThreats(ctx, threatStartTime, endTime, onPageEvents, onChunkComplete)
			if err != nil {
				if ctx.Err() != nil {
					s.logger.Info("S1 threats sync cancelled", zap.String("integrationId", integration.ID))
					continue
				}
				s.logger.Error("Failed to fetch S1 threats", zap.Error(err))
				s.config.UpdateSyncStatus(integration.TenantID, "sentinelone", "error", err.Error())
				continue
			}
			threatCount = count
		} else {
			s.logger.Info("S1 Threats disabled by user settings")
		}

		// à¸”à¸¶à¸‡ Activities (à¸–à¹‰à¸² enabled)
		activityCount := 0
		if fetchSettings.Activities == nil || fetchSettings.Activities.Enabled {
			activityDays := 120
			if fetchSettings.Activities != nil {
				activityDays = fetchSettings.Activities.Days
			}

			activityStartTime := startTime
			if isFullSync {
				activityStartTime = endTime.AddDate(0, 0, -activityDays)
			}

			s.logger.Info("Fetching S1 activities",
				zap.Int("days", activityDays),
				zap.Time("from", activityStartTime))

			count, err := s1Client.FetchActivities(ctx, activityStartTime, endTime, nil, onPageEvents, onChunkComplete)
			if err != nil {
				if ctx.Err() != nil {
					s.logger.Info("S1 activities sync cancelled", zap.String("integrationId", integration.ID))
					continue
				}
				s.logger.Error("Failed to fetch S1 activities", zap.Error(err))
			}
			activityCount = count
		} else {
			s.logger.Info("S1 Activities disabled by user settings")
		}

		// â­ à¸”à¸¶à¸‡ Cloud Detection Alerts (à¸–à¹‰à¸² enabled)
		alertCount := 0
		if fetchSettings.Alerts == nil || fetchSettings.Alerts.Enabled {
			alertDays := 365
			if fetchSettings.Alerts != nil {
				alertDays = fetchSettings.Alerts.Days
			}

			alertStartTime := startTime
			if isFullSync {
				alertStartTime = endTime.AddDate(0, 0, -alertDays)
			}

			s.logger.Info("Fetching S1 cloud detection alerts",
				zap.Int("days", alertDays),
				zap.Time("from", alertStartTime))

			count, err := s1Client.FetchAlerts(ctx, alertStartTime, endTime, onPageEvents, onChunkComplete)
			if err != nil {
				if ctx.Err() != nil {
					s.logger.Info("S1 alerts sync cancelled", zap.String("integrationId", integration.ID))
					continue
				}
				s.logger.Error("Failed to fetch S1 alerts", zap.Error(err))
			}
			alertCount = count
		} else {
			s.logger.Info("S1 Alerts disabled by user settings")
		}

		totalEvents := threatCount + activityCount + alertCount

		// Update State à¸œà¹ˆà¸²à¸™ API (PostgreSQL)
		if isFullSync {
			if err := s.state.SetFullSync(integration.TenantID, provider, urlHash); err != nil {
				s.logger.Error("Failed to set full sync state", zap.Error(err))
			}
		}
		if err := s.state.UpdateCheckpoint(integration.TenantID, provider, urlHash, endTime); err != nil {
			s.logger.Error("Failed to save state", zap.Error(err))
		}
		s.config.UpdateSyncStatus(integration.TenantID, "sentinelone", "success", "")

		// â­ Event-based OPTIMIZE: dedupe à¸—à¸±à¸™à¸—à¸µà¸«à¸¥à¸±à¸‡ sync à¹€à¸ªà¸£à¹‡à¸ˆ
		if totalEvents > 0 {
			s.optimizeClickHouse()
		}

		s.logger.Info("S1 collection completed",
			zap.String("tenantId", integration.TenantID),
			zap.Int("events", totalEvents))
	}

	return nil
}

// collectCrowdStrike à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ CrowdStrike à¸—à¸¸à¸ tenant (à¸£à¸­à¸‡à¸£à¸±à¸š Full Sync + Incremental)
func (s *Scheduler) collectCrowdStrike(forceFullSync bool) error {
	s.logger.Info("Starting CrowdStrike collection", zap.Bool("forceFullSync", forceFullSync))

	// à¸”à¸¶à¸‡ integrations à¸ˆà¸²à¸ Elysia
	integrations, err := s.config.FetchIntegrations("crowdstrike")
	if err != nil {
		s.logger.Error("Failed to fetch CrowdStrike integrations", zap.Error(err))
		return err
	}

	for _, integration := range integrations {
		cfg, err := config.ParseCrowdStrikeConfig(integration.Config)
		if err != nil {
			s.logger.Error("Failed to parse CrowdStrike config",
				zap.String("integrationId", integration.ID),
				zap.Error(err))
			continue
		}

		// à¸ªà¸£à¹‰à¸²à¸‡ URL hash à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸Šà¹‡à¸„ data (BaseURL + ClientID = unique à¸•à¹ˆà¸­ integration)
		urlHash := createURLHash(cfg.BaseURL + cfg.ClientID)
		provider := "crowdstrike"

		// à¸à¸³à¸«à¸™à¸” time range
		endTime := time.Now().UTC()
		var startTime time.Time

		// à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ integration à¹ƒà¸«à¸¡à¹ˆ (pending) à¸«à¸£à¸·à¸­ forceFullSync
		needsFullSync := integration.LastSyncStatus == "pending" || forceFullSync

		if needsFullSync {
			// â­ Option C (Best Practice): à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸² URL à¹€à¸”à¸´à¸¡ + à¸¡à¸µ data à¸„à¸£à¸šà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
			foundExisting := false
			dataComplete := false

			if s.clickhouse != nil {
				// à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¸¡à¸µ data à¸ˆà¸²à¸ URL à¸™à¸µà¹‰à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
				existingTimestamp, count, err := s.clickhouse.GetLatestTimestampByURL(integration.TenantID, provider, urlHash)
				if err != nil {
					s.logger.Warn("Failed to check existing CrowdStrike data by URL in ClickHouse", zap.Error(err))
				} else if !existingTimestamp.IsZero() && count > 0 {
					// à¸¡à¸µ data à¸ˆà¸²à¸ URL à¹€à¸”à¸´à¸¡ â†’ à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¸„à¸£à¸šà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
					foundExisting = true

					dataComplete, _, err = s.clickhouse.CheckDataCompleteness(integration.TenantID, provider, 365)
					if err != nil {
						s.logger.Warn("Failed to check CrowdStrike data completeness", zap.Error(err))
					}

					if dataComplete {
						// URL à¹€à¸”à¸´à¸¡ + data à¸„à¸£à¸š â†’ Incremental sync
						startTime = existingTimestamp.Add(1 * time.Second)
						s.logger.Info("CrowdStrike: URL match + data complete â†’ Incremental sync",
							zap.String("tenantId", integration.TenantID),
							zap.String("urlHash", urlHash),
							zap.Uint64("existingCount", count),
							zap.Time("startFrom", startTime))
					} else {
						// URL à¹€à¸”à¸´à¸¡ à¹à¸•à¹ˆ data à¹„à¸¡à¹ˆà¸„à¸£à¸š â†’ Full sync
						foundExisting = false
						s.logger.Info("CrowdStrike: URL match but data incomplete â†’ Full sync",
							zap.String("tenantId", integration.TenantID),
							zap.String("urlHash", urlHash),
							zap.Uint64("existingCount", count))
					}
				}
			}

			if !foundExisting {
				// à¹„à¸¡à¹ˆà¸¡à¸µ data à¸ˆà¸²à¸ URL à¸™à¸µà¹‰ à¸«à¸£à¸·à¸­ data à¹„à¸¡à¹ˆà¸„à¸£à¸š â†’ Full sync 365 à¸§à¸±à¸™
				startTime = endTime.AddDate(0, 0, -365)
				s.logger.Info("CrowdStrike Full Sync (365 days)",
					zap.String("tenantId", integration.TenantID),
					zap.String("urlHash", urlHash),
					zap.String("reason", integration.LastSyncStatus))
			}
		} else {
			// à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š checkpoint à¸ˆà¸²à¸ API (PostgreSQL)
			checkpoint := s.state.GetCheckpoint(integration.TenantID, provider, urlHash)
			if checkpoint != nil && !checkpoint.IsZero() {
				startTime = checkpoint.Add(1 * time.Second) // +1 sec à¹€à¸žà¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸”à¸¶à¸‡ event à¹€à¸”à¸´à¸¡à¸‹à¹‰à¸³
			} else {
				startTime = endTime.AddDate(0, 0, -s.config.LookbackDays)
			}
		}

		s.logger.Info("CrowdStrike time range",
			zap.String("tenantId", integration.TenantID),
			zap.Time("from", startTime),
			zap.Time("to", endTime))

		// à¸ªà¸£à¹‰à¸²à¸‡ client à¸žà¸£à¹‰à¸­à¸¡ Integration info
		integrationName := integration.Name
		if integrationName == "" {
			integrationName = fmt.Sprintf("%s-%s", integration.Provider, integration.ID[:8])
		}
		csClient := crowdstrike.NewCrowdStrikeClient(integration.TenantID, integration.ID, integrationName, cfg, s.logger)

		// â­ à¸ªà¸£à¹‰à¸²à¸‡ context à¸ªà¸³à¸«à¸£à¸±à¸š cancel sync (à¹€à¸¡à¸·à¹ˆà¸­ Integration à¸–à¸¹à¸à¸¥à¸š)
		ctx, cancel := context.WithCancel(context.Background())
		s.syncMu.Lock()
		s.syncContexts[integration.ID] = cancel
		s.syncMu.Unlock()

		// Cleanup context à¹€à¸¡à¸·à¹ˆà¸­à¸ˆà¸š sync
		defer func(id string) {
			s.syncMu.Lock()
			delete(s.syncContexts, id)
			s.syncMu.Unlock()
			cancel()
		}(integration.ID)

		// Capture urlHash and provider for callbacks
		currentURLHash := urlHash
		currentProvider := provider

		// Callback à¸ªà¸³à¸«à¸£à¸±à¸š save checkpoint à¸«à¸¥à¸±à¸‡à¸ˆà¸š sync (à¸œà¹ˆà¸²à¸™ API)
		onChunkComplete := func(chunkEndTime time.Time) {
			if err := s.state.UpdateCheckpoint(integration.TenantID, currentProvider, currentURLHash, chunkEndTime); err != nil {
				s.logger.Error("Failed to save checkpoint", zap.Error(err))
			}
		}

		// Callback à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¹ˆà¸‡ events à¹„à¸› Vector à¸—à¸±à¸™à¸—à¸µà¹à¸•à¹ˆà¸¥à¸° page (Streaming)
		onPageEvents := func(events []models.UnifiedEvent) error {
			return s.publisher.PublishBatch(events, 5000) // â­ Increased from 500 to reduce ClickHouse CPU
		}

		// â­ à¸”à¸¶à¸‡ FetchSettings à¸ˆà¸²à¸ config (User à¸ªà¸²à¸¡à¸²à¸£à¸– custom à¹„à¸”à¹‰)
		csFetchSettings := cfg.FetchSettings
		if csFetchSettings == nil {
			// Default values à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µ fetchSettings
			csFetchSettings = &config.CSFetchSettings{
				Alerts:     &config.FetchSettingItem{Enabled: true, Days: 365},
				Detections: &config.FetchSettingItem{Enabled: true, Days: 365},
				Incidents:  &config.FetchSettingItem{Enabled: true, Days: 365},
			}
		}

		// à¸”à¸¶à¸‡ Alerts (à¸–à¹‰à¸² enabled)
		alertCount := 0
		if csFetchSettings.Alerts == nil || csFetchSettings.Alerts.Enabled {
			alertDays := 365
			if csFetchSettings.Alerts != nil {
				alertDays = csFetchSettings.Alerts.Days
			}

			alertStartTime := startTime
			if needsFullSync {
				alertStartTime = endTime.AddDate(0, 0, -alertDays)
			}

			s.logger.Info("Fetching CrowdStrike alerts",
				zap.Int("days", alertDays),
				zap.Time("from", alertStartTime))

			count, err := csClient.FetchAlerts(ctx, alertStartTime, endTime, onPageEvents, onChunkComplete)
			if err != nil {
				if ctx.Err() != nil {
					s.logger.Info("CrowdStrike alerts sync cancelled", zap.String("integrationId", integration.ID))
					continue
				}
				s.logger.Error("Failed to fetch CrowdStrike alerts", zap.Error(err))
				s.config.UpdateSyncStatus(integration.TenantID, "crowdstrike", "error", err.Error())
				continue
			}
			alertCount = count
		} else {
			s.logger.Info("CrowdStrike Alerts disabled by user settings")
		}

		// â­ à¸”à¸¶à¸‡ Incidents (à¸–à¹‰à¸² enabled)
		incidentCount := 0
		if csFetchSettings.Incidents == nil || csFetchSettings.Incidents.Enabled {
			incidentDays := 365
			if csFetchSettings.Incidents != nil {
				incidentDays = csFetchSettings.Incidents.Days
			}

			incidentStartTime := startTime
			if needsFullSync {
				incidentStartTime = endTime.AddDate(0, 0, -incidentDays)
			}

			s.logger.Info("Fetching CrowdStrike incidents",
				zap.Int("days", incidentDays),
				zap.Time("from", incidentStartTime))

			count, err := csClient.FetchIncidents(ctx, incidentStartTime, endTime, onPageEvents, onChunkComplete)
			if err != nil {
				if ctx.Err() != nil {
					s.logger.Info("CrowdStrike incidents sync cancelled", zap.String("integrationId", integration.ID))
					continue
				}
				s.logger.Error("Failed to fetch CrowdStrike incidents", zap.Error(err))
			}
			incidentCount = count
		} else {
			s.logger.Info("CrowdStrike Incidents disabled by user settings")
		}

		totalEvents := alertCount + incidentCount

		// à¸­à¸±à¸žà¹€à¸”à¸— checkpoint à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¸œà¹ˆà¸²à¸™ API
		if err := s.state.UpdateCheckpoint(integration.TenantID, provider, urlHash, endTime); err != nil {
			s.logger.Error("Failed to save state", zap.Error(err))
		}
		s.config.UpdateSyncStatus(integration.TenantID, "crowdstrike", "success", "")

		// â­ Event-based OPTIMIZE: dedupe à¸—à¸±à¸™à¸—à¸µà¸«à¸¥à¸±à¸‡ sync à¹€à¸ªà¸£à¹‡à¸ˆ
		if totalEvents > 0 {
			s.optimizeClickHouse()
		}

		s.logger.Info("CrowdStrike collection completed",
			zap.String("tenantId", integration.TenantID),
			zap.Int("alerts", alertCount),
			zap.Int("incidents", incidentCount),
			zap.Int("total", totalEvents))
	}

	return nil
}

// optimizeClickHouse à¸£à¸±à¸™ OPTIMIZE TABLE à¹€à¸žà¸·à¹ˆà¸­ dedupe à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‹à¹‰à¸³ (Event-based)
func (s *Scheduler) optimizeClickHouse() {
	if s.clickhouse == nil {
		return
	}
	s.logger.Info("Running OPTIMIZE TABLE for deduplication (Event-based)")
	if err := s.clickhouse.OptimizeTable("zcrai.security_events"); err != nil {
		s.logger.Error("Failed to optimize security_events", zap.Error(err))
	}
}
